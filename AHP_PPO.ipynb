{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "AHP_PPO.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsYkr2dTir7N",
        "colab_type": "text"
      },
      "source": [
        "## Install Prerequisities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUY1pX7Lirm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfzRVR3yjTnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82c17808-032e-4390-e9c7-f9e3271c4d97"
      },
      "source": [
        "# !wget https://d1b10bmlvqabco.cloudfront.net/attach/k0cxuwlsoj36aw/j4h32wykj7b1l5/k2kzy5zutl61/mjkey.txt\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mjkey.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNAB0pacivJ9",
        "colab_type": "text"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AljikykTR9OS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzR4TeoSR9Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7fqSYhWSM9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4d98ac84-720d-4501-bfc3-0845f54a2163"
      },
      "source": [
        "!git clone https://github.com/DAC-Prime/supreme-waffle.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'supreme-waffle'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 45 (delta 15), reused 29 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1RHJB3fS3FM",
        "colab_type": "text"
      },
      "source": [
        "## Create Environments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlBo5fSUSaf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from env import \n",
        "\n",
        "env = gym.make()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Log48mH-S6fJ",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIgYWz1LS576",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)   \n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        \n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_outputs),\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
        "        \n",
        "        self.apply(init_weights)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        value = self.critic(x)\n",
        "        mu    = self.actor(x)\n",
        "        std   = self.log_std.exp().expand_as(mu)\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value\n",
        "\n",
        "def plot(frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "    \n",
        "def test_env(vis=False):\n",
        "    state = env.reset()\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
        "        state = next_state\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go0TrbvRTKIc",
        "colab_type": "text"
      },
      "source": [
        "## PPO Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixa3auxOTJ3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    ids = np.random.permutation(batch_size)\n",
        "    ids = np.split(ids[:batch_size // mini_batch_size * mini_batch_size], batch_size // mini_batch_size)\n",
        "    for i in range(len(ids)):\n",
        "        yield states[ids[i], :], actions[ids[i], :], log_probs[ids[i], :], returns[ids[i], :], advantage[ids[i], :]      \n",
        "\n",
        "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "            dist, value = model(state)\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            ratio = (new_log_probs - old_log_probs).exp()\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdCMt2P8TRQF",
        "colab_type": "text"
      },
      "source": [
        "## Training Set Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIHn55KhTTag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_inputs  = envs.observation_space.shape[0]\n",
        "num_outputs = envs.action_space.shape[0]\n",
        "\n",
        "#Hyper params:\n",
        "threshold_reward = -200\n",
        "\n",
        "hidden_size      = 32\n",
        "lr               = 1e-3\n",
        "num_steps        = 128\n",
        "mini_batch_size  = 256\n",
        "ppo_epochs       = 30\n",
        "\n",
        "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "max_frames = 15000\n",
        "frame_idx  = 0\n",
        "test_rewards = []\n",
        "\n",
        "state = envs.reset()\n",
        "early_stop = False\n",
        "\n",
        "while frame_idx < max_frames and not early_stop:\n",
        "\n",
        "    log_probs = []\n",
        "    values    = []\n",
        "    states    = []\n",
        "    actions   = []\n",
        "    rewards   = []\n",
        "    masks     = []\n",
        "    entropy = 0\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        dist, value = model(state)\n",
        "\n",
        "        action = dist.sample()\n",
        "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy += dist.entropy().mean()\n",
        "        \n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
        "        \n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        \n",
        "        state = next_state\n",
        "        frame_idx += 1\n",
        "        \n",
        "        if frame_idx % 1000 == 0:\n",
        "            test_reward = np.mean([test_env() for _ in range(10)])\n",
        "            test_rewards.append(test_reward)\n",
        "            plot(frame_idx, test_rewards)\n",
        "            if test_reward > threshold_reward: early_stop = True\n",
        "            \n",
        "\n",
        "    next_state = torch.FloatTensor(next_state).to(device)\n",
        "    _, next_value = model(next_state)\n",
        "    returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "    returns   = torch.cat(returns).detach()\n",
        "    log_probs = torch.cat(log_probs).detach()\n",
        "    values    = torch.cat(values).detach()\n",
        "    states    = torch.cat(states)\n",
        "    actions   = torch.cat(actions)\n",
        "    advantage = returns - values\n",
        "    \n",
        "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}